{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build a text processing pipeline with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Installation\n",
    "- Download pre-trained Models\n",
    "- Doc/Token/Span\n",
    "- Token Extracting / Removing / Transforming (Tokenization, Lemmatization)\n",
    "- Sentence Segmentation\n",
    "- Part of Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Dependency Parsing\n",
    "- Word Vectors\n",
    "- Sentence Similarity\n",
    "- Customize pipeline components\n",
    "- How does pipeline works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Installation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- spaCy is compatible with 64-bit CPython 2.7 / 3.5+ \n",
    "- Runs on Unix/Linux, macOS/OS X and Windows\n",
    "- The latest spaCy releases are available over pip and conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The easiest way to download a model is via spaCy’s download command.  \n",
    "\n",
    "It takes care of finding the best-matching model compatible with your spaCy installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "spaCy’s models can also be installed as Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# With external URL\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
    "\n",
    "# With local file\n",
    "# !pip install /Users/you/en_core_web_sm-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Configuration\n",
    "- Binary weights\n",
    "- Lexical entries\n",
    "- Data files\n",
    "- Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    Directory structure\n",
    "    ├── en_core_web_sm-2.2.5\n",
    "    │   ├── accuracy.json\n",
    "    │   ├── meta.json\n",
    "    │   ├── ner\n",
    "    │   │   ├── cfg\n",
    "    │   │   ├── model\n",
    "    │   │   └── moves\n",
    "    │   ├── parser\n",
    "    │   ├── tagger\n",
    "    │   ├── tokenizer\n",
    "    │   └── vocab\n",
    "    │       ├── key2row\n",
    "    │       ├── lexemes.bin\n",
    "    │       ├── lookups.bin\n",
    "    │       ├── strings.json\n",
    "    │       └── vectors\n",
    "    ├── __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doc/Token/Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "first_token = doc[0]\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[0:2]\n",
    "[token.text for token in span]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### using white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Rare bird’s detection highlights promise of ‘environmental DNA’\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare\n",
      "bird’s\n",
      "detection\n",
      "highlights\n",
      "promise\n",
      "of\n",
      "‘environmental\n",
      "DNA’\n"
     ]
    }
   ],
   "source": [
    "token_lsit = text.split()\n",
    "for token in token_lsit:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare\n",
      "bird\n",
      "’s\n",
      "detection\n",
      "highlights\n",
      "promise\n",
      "of\n",
      "‘\n",
      "environmental\n",
      "DNA\n",
      "’\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lemmatization is the process of reducing inflected forms, sometimes derivationally related forms of a word to a common base form. This reduced form or root word is called a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'be', 'be']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"am are is\"\n",
    "[token.lemma_ for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:look -> lemma:look\n",
      "token:looks -> lemma:look\n",
      "token:looked -> lemma:look\n"
     ]
    }
   ],
   "source": [
    "text = \"look looks looked\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Syracuse University has never been bound by convention.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:Syracuse -> lemma:Syracuse\n",
      "token:University -> lemma:University\n",
      "token:has -> lemma:have\n",
      "token:never -> lemma:never\n",
      "token:been -> lemma:be\n",
      "token:bound -> lemma:bind\n",
      "token:by -> lemma:by\n",
      "token:convention -> lemma:convention\n",
      "token:. -> lemma:.\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "text = '''Since 1870, Syracuse University has upheld the idea that “brains and heart shall have a fair chance” at achieving a University degree. We were pioneers in recognizing women’s achievements, the academic validity of the fine arts, the emergence of information studies and the importance of educational opportunities that serve veterans. Our founding values are embedded in 150 years of educational innovation, expansion, discovery and change. See a timeline of events in Syracuse University's 150 years of history.'''\n",
    "doc = nlp(text)\n",
    "words =  [token.text for token in doc if token.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 6), ('the', 5), ('University', 3), ('and', 3), ('a', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter = Counter(words)\n",
    "word_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Token Extracting / Removing / Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "|Attribute Name\t|Type|Description                                                                    |\n",
    "|:--------------------:|:---------:|:------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| lemma              | int     | Base form of the token, with no inflectional suffixes.                                                                             |\n",
    "| lemma_             | unicode | Base form of the token, with no inflectional suffixes.                                                                             |\n",
    "| norm               | int     | The token’s norm, i.e. a normalized form of the token text. Usually set in the language’s tokenizer exceptions or norm exceptions. |\n",
    "| norm_              | unicode | The token’s norm, i.e. a normalized form of the token text. Usually set in the language’s tokenizer exceptions or norm exceptions. |\n",
    "| lower              | int     | Lowercase form of the token.                                                                                                       |\n",
    "| lower_             | unicode | Lowercase form of the token text. Equivalent to Token.text.lower().                                                                |\n",
    "| shape              | int     | Transform of the tokens’s string, to show orthographic features. For example, “Xxxx” or “dd”.                                      |\n",
    "| shape_             | unicode | Transform of the tokens’s string, to show orthographic features. For example, “Xxxx” or “dd”.                                      |\n",
    "| prefix             | int     | Hash value of a length-N substring from the start of the token. Defaults to N=1.                                                   |\n",
    "| prefix_            | unicode | A length-N substring from the start of the token. Defaults to N=1.                                                                 |\n",
    "| suffix             | int     | Hash value of a length-N substring from the end of the token. Defaults to N=3.                                                     |\n",
    "| suffix_            | unicode | Length-N substring from the end of the token. Defaults to N=3.                                                                     |\n",
    "| is_alpha           | bool    | Does the token consist of alphabetic characters? Equivalent to token.text.isalpha().                                               |\n",
    "| is_ascii           | bool    | Does the token consist of ASCII characters? Equivalent to all(ord(c) < 128 for c in token.text).                                   |\n",
    "| is_digit           | bool    | Does the token consist of digits? Equivalent to token.text.isdigit().                                                              |\n",
    "| is_lower           | bool    | Is the token in lowercase? Equivalent to token.text.islower().                                                                     |\n",
    "| is_upper           | bool    | Is the token in uppercase? Equivalent to token.text.isupper().                                                                     |\n",
    "| is_title           | bool    | Is the token in titlecase? Equivalent to token.text.istitle().                                                                     |\n",
    "| is_punct           | bool    | Is the token punctuation?                                                                                                          |\n",
    "| is_left_punct      | bool    | Is the token a left punctuation mark, e.g. (?                                                                                      |\n",
    "| is_right_punct     | bool    | Is the token a right punctuation mark, e.g. )?                                                                                     |\n",
    "| is_space           | bool    | Does the token consist of whitespace characters? Equivalent to token.text.isspace().                                               |\n",
    "| is_bracket         | bool    | Is the token a bracket?                                                                                                            |\n",
    "| is_quote           | bool    | Is the token a quotation mark?                                                                                                     |\n",
    "| is_currency V2.0.8 | bool    | Is the token a currency symbol?                                                                                                    |\n",
    "| like_url           | bool    | Does the token resemble a URL?                                                                                                     |\n",
    "| like_num           | bool    | Does the token represent a number? e.g. “10.9”, “10”, “ten”, etc.                                                                  |\n",
    "| like_email         | bool    | Does the token resemble an email address?                                                                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get all the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text='''The future opens wide for those who can master our ever-evolving,\n",
    "data-driven world – come learn that mastery from the experts\n",
    "at the Syracuse University iSchool!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      ",\n",
      "-\n",
      "–\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "num_list=[]\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        num_list.append(token)\n",
    "for punc in num_list:\n",
    "    print(punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Removing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twenty', 'front', \"'re\", 'n‘t', 'another', 'mostly', 'am', 'through']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_stopwords)[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[quick, brown, fox, jumps, lazy, dog]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lowercased = [ token.lower_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cols = (\"text\", \"lemma_\",\"is_punct\", \"is_stop\", \"is_alpha\", \"is_space\", \"lower_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rows = [] \n",
    "for t in doc:\n",
    "    row = [t.text, t.lemma_,  t.is_punct,  t.is_stop,  t.is_alpha,  t.is_space,  t.lower_]\n",
    "    rows.append(row)\n",
    "attri_pdf = pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_space</th>\n",
       "      <th>lower_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quick</td>\n",
       "      <td>quick</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fox</td>\n",
       "      <td>fox</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jumps</td>\n",
       "      <td>jump</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>jumps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>over</td>\n",
       "      <td>over</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lazy</td>\n",
       "      <td>lazy</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>lazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text lemma_  is_punct  is_stop  is_alpha  is_space lower_\n",
       "0    The    the     False     True      True     False    the\n",
       "1  quick  quick     False    False      True     False  quick\n",
       "2  brown  brown     False    False      True     False  brown\n",
       "3    fox    fox     False    False      True     False    fox\n",
       "4  jumps   jump     False    False      True     False  jumps\n",
       "5   over   over     False     True      True     False   over\n",
       "6    the    the     False     True      True     False    the\n",
       "7   lazy   lazy     False    False      True     False   lazy\n",
       "8    dog    dog     False    False      True     False    dog"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attri_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = '''In some cases, eDNA analyses are being used to enforce policy. For example, in 2014, the UK government approved the use of eDNA analysis for detecting the endangered great crested newt in land-use surveys that are required by law.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In some cases, eDNA analyses are being used to enforce policy. For example, in 2014, the UK government approved the use of eDNA analysis for detecting the endangered great crested newt in land-use surveys that are required by law.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_pos=0, end_pos=13, text:In some cases, eDNA analyses are being used to enforce policy.\n",
      "start_pos=13, end_pos=46, text:For example, in 2014, the UK government approved the use of eDNA analysis for detecting the endangered great crested newt in land-use surveys that are required by law.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\"start_pos={}, end_pos={}, text:{}\".format(sent.start, sent.end, sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT DET determiner\n",
      "quick JJ ADJ adjective\n",
      "brown JJ ADJ adjective\n",
      "fox NN NOUN noun, singular or mass\n",
      "jumps NNS NOUN noun, plural\n",
      "over IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "lazy JJ ADJ adjective\n",
      "dog NN NOUN noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The/DT <--det-- jumps/NNS\n",
      "quick/JJ <--amod-- jumps/NNS\n",
      "brown/JJ <--amod-- jumps/NNS\n",
      "fox/NN <--compound-- jumps/NNS\n",
      "jumps/NNS <--ROOT-- jumps/NNS\n",
      "over/IN <--prep-- jumps/NNS\n",
      "the/DT <--det-- dog/NN\n",
      "lazy/JJ <--amod-- dog/NN\n",
      "dog/NN <--pobj-- over/IN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4d637a9d1c214ac2b7a74d33d2cdfeb0-0\" class=\"displacy\" width=\"860\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">dog</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,2.0 410.0,2.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,47.0 405.0,47.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,184.0 L152,172.0 168,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-2\" stroke-width=\"2px\" d=\"M250,182.0 C250,92.0 400.0,92.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,184.0 L242,172.0 258,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-3\" stroke-width=\"2px\" d=\"M340,182.0 C340,137.0 395.0,137.0 395.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,184.0 L332,172.0 348,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-4\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M485.0,184.0 L493.0,172.0 477.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-5\" stroke-width=\"2px\" d=\"M610,182.0 C610,92.0 760.0,92.0 760.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,184.0 L602,172.0 618,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-6\" stroke-width=\"2px\" d=\"M700,182.0 C700,137.0 755.0,137.0 755.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M700,184.0 L692,172.0 708,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-7\" stroke-width=\"2px\" d=\"M520,182.0 C520,47.0 765.0,47.0 765.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d637a9d1c214ac2b7a74d33d2cdfeb0-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M765.0,184.0 L773.0,172.0 757.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| TAG | ID | DESCRIPTION                           |\n",
    "|:-----:|:----:|:---------------------------------------:|\n",
    "| I   | 1  | Token is inside an entity.            |\n",
    "| O   | 2  | Token is outside an entity.           |\n",
    "| B   | 3  | Token begins an entity.               |\n",
    "|     | 0  | No entity tag is set (missing value). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text='''We’re bringing the celebration of Syracuse University’s 150 years of impact to Chicago'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’re bringing the celebration of Syracuse University’s 150 years of impact to Chicago\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syracuse University, [34,53), ORG\n",
      "    Syracuse B ORG\n",
      "    University I ORG\n",
      "150 years, [56,65), DATE\n",
      "    150 B DATE\n",
      "    years I DATE\n",
      "Chicago, [79,86), GPE\n",
      "    Chicago B GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(\"{}, [{},{}), {}\".format(ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "    for token in ent.as_doc():        \n",
    "        print(\"    {} {} {}\".format(token, token.ent_iob_, token.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __text__ gives the Unicode text representation of the entity.\n",
    "- __start_char__ denotes the character offset for the start of the entity.\n",
    "- __end_char__ denotes the character offset for the end of the entity.\n",
    "- __label___ gives the label of the entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">We’re bringing the celebration of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Syracuse University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    150 years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of impact to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chicago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_lg\n",
    "# Doc.similarity(), Span.similarity() and Token.similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "\n",
    "apple = nlp_lg.vocab[\"apple\"]\n",
    "banana = nlp_lg.vocab[\"banana\"]\n",
    "car = nlp_lg.vocab[\"car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6391e-01,  4.3771e-01, -2.0447e-01, -2.2889e-01, -1.4227e-01,\n",
       "        2.7396e-01, -1.1435e-02, -1.8578e-01,  3.7361e-01,  7.5339e-01,\n",
       "       -3.0591e-01,  2.3741e-02, -7.7876e-01, -1.3802e-01,  6.6992e-02,\n",
       "       -6.4303e-02, -4.0024e-01,  1.5309e+00, -1.3897e-02, -1.5657e-01,\n",
       "        2.5366e-01,  2.1610e-01, -3.2720e-01,  3.4974e-01, -6.4845e-02,\n",
       "       -2.9501e-01, -6.3923e-01, -6.2017e-02,  2.4559e-01, -6.9334e-02,\n",
       "       -3.9967e-01,  3.0925e-02,  4.9033e-01,  6.7524e-01,  1.9481e-01,\n",
       "        5.1488e-01, -3.1149e-01, -7.9939e-02, -6.2096e-01, -5.3277e-03,\n",
       "       -1.1264e-01,  8.3528e-02, -7.6947e-03, -1.0788e-01,  1.6628e-01,\n",
       "        4.2273e-01, -1.9009e-01, -2.9035e-01,  4.5630e-02,  1.0120e-01,\n",
       "       -4.0855e-01, -3.5000e-01, -3.6175e-01, -4.1396e-01,  5.9485e-01,\n",
       "       -1.1524e+00,  3.2424e-02,  3.4364e-01, -1.9209e-01,  4.3255e-02,\n",
       "        4.9227e-02, -5.4258e-01,  9.1275e-01,  2.9576e-01,  2.3658e-02,\n",
       "       -6.8737e-01, -1.9503e-01, -1.1059e-01, -2.2567e-01,  2.4180e-01,\n",
       "       -3.1230e-01,  4.2700e-01,  8.3952e-02,  2.2703e-01,  3.0581e-01,\n",
       "       -1.7276e-01,  3.2536e-01,  5.4696e-03, -3.2745e-01,  1.9439e-01,\n",
       "        2.2616e-01,  7.4742e-02,  2.2033e-01, -4.0301e-01, -3.1594e-01,\n",
       "       -2.8910e-02,  9.7858e-01,  7.1860e-01,  1.4995e-01,  6.3421e-02,\n",
       "        2.8332e-01, -1.5231e-01,  3.9330e-04,  1.8076e-01, -4.0199e-01,\n",
       "        6.0187e-02, -2.7543e-02,  1.6590e-01, -2.5774e-01,  1.6150e-01,\n",
       "        3.7247e-01, -3.8273e-01,  2.4012e-01, -4.2617e-02, -6.6785e-01,\n",
       "       -9.4437e-01,  2.7916e-01,  1.0476e-01,  1.3952e+00, -1.4296e-01,\n",
       "       -5.5049e-01,  5.3982e-02, -7.7524e-01, -2.8255e-01, -2.3323e-02,\n",
       "        2.4801e-01,  2.2855e-01, -3.7408e-01,  7.6012e-02,  2.4031e-01,\n",
       "        1.0746e-01,  1.2411e-01, -2.0676e-01, -2.5804e-01, -1.6791e-01,\n",
       "        4.3499e-01,  6.1762e-01, -2.9955e-02,  1.6196e-01, -2.9001e-01,\n",
       "       -3.1159e-01, -8.7262e-01,  4.3167e-01, -1.5071e-01, -4.1420e-01,\n",
       "       -5.3730e-01, -1.9910e-01,  1.3270e-01, -1.5018e-01, -4.9335e-01,\n",
       "       -2.5127e+00,  3.1660e-01,  3.6396e-01, -5.9248e-02,  3.1120e-02,\n",
       "        4.1071e-02,  1.6917e-02,  5.8410e-01, -2.0201e-01,  7.0238e-02,\n",
       "        8.7547e-01, -2.0114e-01,  5.1920e-01,  2.6786e-01, -5.5643e-01,\n",
       "       -3.1247e-01, -3.7992e-01,  4.2857e-01,  4.1780e-01,  3.0608e-01,\n",
       "       -2.1657e-01,  7.2464e-01,  6.1734e-01,  5.8085e-02, -6.2708e-01,\n",
       "        5.2895e-02, -2.5628e-01, -3.2688e-01, -6.1280e-01,  6.2609e-01,\n",
       "       -1.7965e-01,  8.8925e-01,  2.1963e-01, -3.4052e-03, -7.8663e-02,\n",
       "        3.4799e-01, -2.6062e-01,  8.0410e-03,  1.1721e-01, -4.5147e-01,\n",
       "       -1.2178e-01, -5.7030e-01,  4.6602e-01,  2.5059e-02,  5.3986e-02,\n",
       "       -7.6693e-01,  1.3173e-01, -2.8776e-02, -4.1915e-01, -2.4415e-01,\n",
       "       -4.0295e-01, -4.1520e-01,  3.7643e-02, -1.4843e-01,  2.6094e-02,\n",
       "        1.5315e-01,  3.8310e-01, -5.5825e-01, -3.3433e-01, -2.7939e-02,\n",
       "       -4.3712e-01, -3.1802e-01, -3.1731e-01,  9.2891e-02, -9.9397e-02,\n",
       "       -1.8846e-01,  5.2270e-02,  2.9061e-01,  1.0639e+00,  9.9584e-02,\n",
       "       -5.6775e-01,  2.9446e-01,  3.7797e-01, -2.1905e-01, -5.2616e-01,\n",
       "       -4.1744e-01, -6.5951e-01, -4.0820e-01, -6.0945e-01,  1.1759e-02,\n",
       "       -2.9122e-01, -3.1457e-01,  5.7076e-02,  4.1503e-01,  3.7345e-01,\n",
       "       -4.7119e-02, -7.1996e-02,  1.4587e-01, -3.0763e-01,  1.0759e-01,\n",
       "       -5.9447e-01, -4.0205e-01,  3.0677e-01, -1.9891e-01, -7.0775e-01,\n",
       "       -1.1513e-01,  3.0866e-01, -6.9235e-01,  2.1219e-01,  1.0554e-01,\n",
       "        2.2617e-01, -2.6145e-01, -3.9298e-01, -2.3585e-01,  3.0795e-02,\n",
       "       -1.0193e-01,  3.2070e-01,  3.0505e-01, -5.3470e-01, -7.9272e-02,\n",
       "       -1.6817e-01, -2.2115e-01, -3.5143e-01, -9.2376e-02,  1.4686e-01,\n",
       "       -1.9859e-01,  2.0460e-01,  2.0276e-01,  3.6144e-01, -3.5867e-01,\n",
       "        4.0095e-01,  6.3686e-02, -1.2763e-01, -1.6226e-01, -3.1763e-01,\n",
       "       -5.8732e-01, -5.4009e-01, -4.9035e-01, -4.6035e-01, -1.9794e-01,\n",
       "       -2.5209e-01,  2.5706e-01,  4.0110e-01,  5.2830e-02, -3.2079e-01,\n",
       "        3.9563e-01, -4.4512e-01, -9.1862e-02, -1.9243e-01,  1.5397e-01,\n",
       "       -2.8923e-01,  6.0561e-01,  5.8133e-01,  3.2268e-01,  6.3892e-02,\n",
       "        8.5438e-02,  1.4956e-01,  3.8134e-01, -1.1820e-01, -2.3951e-01,\n",
       "       -6.7731e-01,  2.8090e-01, -5.1770e-01, -4.1098e-01, -4.1292e-01,\n",
       "       -6.7856e-02, -3.3721e-02, -7.2958e-01, -4.7891e-01,  7.2956e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5831844"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.similarity(banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5831844"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana.similarity(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21747091"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.similarity(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine(x,y):\n",
    "    return np.dot(x,y) / (np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "man = nlp_lg.vocab[\"man\"].vector\n",
    "woman = nlp_lg.vocab[\"woman\"].vector\n",
    "king = nlp_lg.vocab[\"king\"].vector\n",
    "queen = nlp_lg.vocab[\"queen\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7880845"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(king-man+woman,queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, Token.vector returns the vector for its underlying Lexeme, while Doc.vector and Span.vector return an average of the vectors of their tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp_lg(\"The quick brown fox jumps over the lazy dog\")\n",
    "doc2 = nlp_lg(\"The lazy dog jumps over the quick brown fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.similarity(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1_vec=doc1.vector\n",
    "doc2_vec=doc2.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(doc2_vec,doc1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(doc2_vec,doc1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc3 = nlp(\"I like snow\")\n",
    "doc4 = nlp(\"I hate snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85214496"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(doc3.vector,doc4.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img style=\"text-align:center;\" src=\"img/pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| NAME      | COMPONENT         | CREATES                                             | DESCRIPTION                                      |\n",
    "|:-----------|:-------------------|:-----------------------------------------------------|:--------------------------------------------------|\n",
    "| tokenizer | Tokenizer         | Doc                                                 | Segment text into tokens.                        |\n",
    "| tagger    | Tagger            | Doc[i].tag                                          | Assign part-of-speech tags.                      |\n",
    "| parser    | DependencyParser  | Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks | Assign dependency labels.                        |\n",
    "| ner       | EntityRecognizer  | Doc.ents, Doc[i].ent_iob, Doc[i].ent_type           | Detect and label named entities.                 |\n",
    "| textcat   | TextCategorizer   | Doc.cats                                            | Assign document labels.                          |\n",
    "| …         | custom components | Doc._.xxx, Token._.xxx, Span._.xxx                  | Assign custom attributes, methods or properties. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does pipeline works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spaCy model consists of three components: \n",
    "- the weights, i.e. binary data loaded in from a directory, \n",
    "- a pipeline of functions called in order, and \n",
    "- language data like the tokenization rules and annotation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "      # meta.json\n",
    "      {\n",
    "        \"lang\":\"en\",\n",
    "        \"name\":\"core_web_sm\",\n",
    "        \"pipeline\":[\"tagger\", \"parser\", \"ner\"],\n",
    "        ...\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    lang = \"en\"\n",
    "    pipeline = [\"tagger\", \"parser\", \"ner\"]\n",
    "    data_path = \"path/to/en_core_web_sm/en_core_web_sm-2.0.0\"\n",
    "\n",
    "    cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
    "    nlp = cls()                             # 2. Initialize it\n",
    "    for name in pipeline:\n",
    "        component = nlp.create_pipe(name)   # 3. Create the pipeline components\n",
    "        nlp.add_pipe(component)             # 4. Add the component to the pipeline\n",
    "    nlp.from_disk(model_data_path)          # 5. Load in the binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Language(object):\n",
    "    def __init__(\n",
    "        self, vocab=True, make_doc=True, max_length=10 ** 6, meta={}, **kwargs\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = make_doc\n",
    "        self.pipeline = []\n",
    "        self.max_length = max_length\n",
    "        self._optimizer = None\n",
    "    def make_doc(self, text):\n",
    "        return self.tokenizer(text)\n",
    "    def pipe_names(self):\n",
    "        return [pipe_name for pipe_name, _ in self.pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def add_pipe(\n",
    "        self, component, name=None, before=None, after=None, first=None, last=None\n",
    "    ):\n",
    "        pipe = (name, component)\n",
    "        if last or not any([first, before, after]):\n",
    "            self.pipeline.append(pipe)\n",
    "        elif first:\n",
    "            self.pipeline.insert(0, pipe)\n",
    "        elif before and before in self.pipe_names:\n",
    "            self.pipeline.insert(self.pipe_names.index(before), pipe)\n",
    "        elif after and after in self.pipe_names:\n",
    "            self.pipeline.insert(self.pipe_names.index(after) + 1, pipe)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                Errors.E001.format(name=before or after, opts=self.pipe_names)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def __call__(self, text, disable=[], component_cfg=None):\n",
    "        doc = self.make_doc(text)\n",
    "        for name, proc in self.pipeline:\n",
    "            if name in disable:\n",
    "                continue\n",
    "            if not hasattr(proc, \"__call__\"):\n",
    "                raise ValueError(Errors.E003.format(component=type(proc), name=name))\n",
    "            doc = proc(doc, **component_cfg.get(name, {}))\n",
    "            if doc is None:\n",
    "                raise ValueError(Errors.E005.format(name=name))\n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f3d3c46c2e8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f3d06ae4be8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f3d06ae4c48>)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disabling and modifying pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.rename_pipe(\"ner\", \"entityrecognizer\")\n",
    "nlp.replace_pipe(\"tagger\", my_custom_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Customize pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A component receives a Doc object and can modify it. By adding a component to the pipeline, you’ll get access to the Doc at any point during processing – instead of only being able to modify it afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| ARGUMENT | TYPE | DESCRIPTION                                          |\n",
    "|----------|------|------------------------------------------------------|\n",
    "| doc      | Doc  | The Doc object processed by the previous component.  |\n",
    "| RETURNS  | Doc  | The Doc object processed by this pipeline component. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| ARGUMENT | TYPE    | DESCRIPTION                                                        |\n",
    "|----------|---------|--------------------------------------------------------------------|\n",
    "| last     | bool    | If set to True, component is added last in the pipeline (default). |\n",
    "| first    | bool    | If set to True, component is added first in the pipeline.          |\n",
    "| before   | unicode | String name of component to add the new component before.          |\n",
    "| after    | unicode | String name of component to add the new component after.           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stopwords_removal', 'tagger', 'parser', 'ner']\n",
      "Before stopwords_removal, this doc is: This is a sentence.\n",
      "After stopwords_removal, this doc is: sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import json\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    print(\"Before stopwords_removal, this doc is: {}\".format(doc))\n",
    "    space_list = [t.whitespace_  for t in doc if not t.is_stop]\n",
    "    new_doc = Doc(doc.vocab,\n",
    "              words=[t.orth_ for t in doc if not t.is_stop],\n",
    "              spaces=space_list\n",
    "              )\n",
    "    return new_doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords_removal\", first=True)\n",
    "print(nlp.pipe_names)  # ['stopwords_removal', 'tagger', 'parser', 'ner']\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print(\"After stopwords_removal, this doc is: {}\".format(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stopwords_removal', 'tagger', 'parser', 'ner']\n",
      "Before stopwords_removal, this doc is: This is a sentence.\n",
      "After stopwords_removal, this doc is: sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import json\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    print(\"Before stopwords_removal, this doc is: {}\".format(doc))\n",
    "    space_list = [t.whitespace_  for t in doc if not t.is_stop]\n",
    "    new_doc = Doc(doc.vocab,\n",
    "              words=[t.orth_ for t in doc if not t.is_stop],\n",
    "              spaces=space_list\n",
    "              )\n",
    "    return new_doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords_removal\", before='tagger')\n",
    "print(nlp.pipe_names)  # ['stopwords_removal', 'tagger', 'parser', 'ner']\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print(\"After stopwords_removal, this doc is: {}\".format(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'stopwords_removal']\n",
      "Before stopwords_removal, this doc is: This is a sentence.\n",
      "After stopwords_removal, this doc is: sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import json\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    print(\"Before stopwords_removal, this doc is: {}\".format(doc))\n",
    "    space_list = [t.whitespace_  for t in doc if not t.is_stop]\n",
    "    new_doc = Doc(doc.vocab,\n",
    "              words=[t.orth_ for t in doc if not t.is_stop],\n",
    "              spaces=space_list\n",
    "              )\n",
    "    return new_doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords_removal\", after='ner')\n",
    "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'stopwords_removal']\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print(\"After stopwords_removal, this doc is: {}\".format(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attribute extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "Doc.set_extension(\"hello\", default=True)\n",
    "assert doc._.hello\n",
    "doc._.hello = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def recovery_xref_inner_text(doc):\n",
    "    xref_mapping: Dict = doc._.xref_mapping\n",
    "    new_doc[in_pos]._.rid = rid\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Property extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Doc.set_extension(\"hello\", getter=get_hello_value, setter=set_hello_value)\n",
    "assert doc._.hello\n",
    "doc._.hello = \"Hi!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Method extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Doc.set_extension(\"hello\", method=lambda doc, name: \"Hi {}!\".format(name))\n",
    "assert doc._.hello(\"Bob\") == \"Hi Bob!\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
